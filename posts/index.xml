<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Brad Thornborrow</title><link>/posts/</link><description>Recent content in Posts on Brad Thornborrow</description><generator>Hugo -- gohugo.io</generator><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Thu, 22 Aug 2019 00:00:00 +0000</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>IPC Annual Report Education Resources</title><link>/posts/2019/08/ipc-annual-report-education-resources/</link><pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate><guid>/posts/2019/08/ipc-annual-report-education-resources/</guid><description>Earlier this summer, the Information and Privacy Commissioner (IPC) of Ontario released their 2018 Annual Report . One of the more interesting stories in the report concerned Privacy Order PO-3617. This order related to a lengthy dispute between the Ontario Medical Association (OMA) and Toronto Star, who requested the release of the top 100 OHIP billing physicians in Ontario. In April of this year, the Supreme Court of Canada dismissed the OMA’s appeal and allowed for the release of this information, which many have seen as a victory for government openness and transparency.</description><content type="html"><![CDATA[<p>Earlier this summer, the Information and Privacy Commissioner (IPC) of Ontario released their <a href="https://www.ipc.on.ca/2018-annual-report-privacy-and-accountability-for-a-digital-ontario/" target="_blank">2018 Annual Report</a>
. One of the more interesting stories in the report concerned Privacy Order PO-3617. This order related to a lengthy dispute between the Ontario Medical Association (OMA) and Toronto Star, who requested the release of the top 100 OHIP billing physicians in Ontario. In April of this year, the Supreme Court of Canada dismissed the OMA’s appeal and allowed for the release of this information, which many have seen as a victory for government openness and transparency.</p>
<p>One of the hidden gems in the report is on page 18 where the Commissioner discusses a <a href="https://www.ipc.on.ca/new-lesson-plans-for-educators-privacy-rights-digital-literacy-and-online-safety/" target="_blank">three-volume set</a>
 of lesson plans for educators which the IPC helped develop and release in 2018. While the lesson plans are focused primarily on the K – 12 environment, they are an excellent resource and could also be used in the development of Privacy and Information Security education programs for any organization. Included with the resources are links to additional materials developed by the IPC and the Office of the Privacy Commissioner (OPC) of Canada, such as:</p>
<ul>
<li><a href="https://www.priv.gc.ca/en/about-the-opc/what-we-do/awareness-campaigns-and-events/privacy-education-for-kids/educational-resources-for-teachers/educational-poster/" target="_blank">5 Tips to protect your privacy online</a>
</li>
<li><a href="https://www.priv.gc.ca/en/about-the-opc/what-we-do/awareness-campaigns-and-events/privacy-education-for-kids/social-smarts-privacy-the-internet-and-you/" target="_blank">Social Smarts: privacy, the Internet and you</a>
</li>
<li><a href="https://www.ipc.on.ca/wp-content/uploads/Resources/id-theft-e.pdf" target="_blank">Identity Theft: A Crime of Opportunity</a>
</li>
</ul>
<p>Also noted in the report, the IPC co-chaired a task force with the Office of the Privacy Commissioner of Canada to research, create and sponsor a resolution at the 40th International Conference of Data Protection and Privacy Commissioners. This <a href="https://icdppc.org/wp-content/uploads/2018/11/20180918_ICDPPC-40th_DEWG-Resolution_ADOPTED.pdf" target="_blank">resolution</a>
 includes 24 recommendations and guidance for protecting user privacy when developing, implementing, or using online educational services.</p>
<p>If you have any interest in Privacy, I recommend that you read the <a href="https://www.ipc.on.ca/wp-content/uploads/2019/06/ar-2018-e.pdf" target="_blank">report</a>
 and the education resources I highlighted, especially if you have a role in Education or Organizational Development.</p>
<p>To contact me or provide feedback on this post, please use the <a href="http://bftsystems.ca/contact/" target="_blank">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>Automating Jekyll site updates with GitLab CI Schedules</title><link>/posts/2018/10/automating-jekyll-site-updates-with-gitlab-ci-schedules/</link><pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate><guid>/posts/2018/10/automating-jekyll-site-updates-with-gitlab-ci-schedules/</guid><description>One of the earliest posts I made on this site described the process I used to automate daily updates using a Raspberry Pi server running Raspbian. With my recent move of the site to GitLab Pages , I was able to automate daily site updates without the need of an external server. This was made possible through the magic of the GitLab CI (Continuous Integration) Scheduling.
For those not familiar with GitLab Pages, it uses a CI configuration file (.</description><content type="html"><![CDATA[<p>One of the earliest posts I made on this site described the process I used to automate daily updates using a Raspberry Pi server running Raspbian. With my recent move of the site to <a href="https://about.gitlab.com/features/pages/" target="_blank">GitLab Pages</a>
, I was able to automate daily site updates without the need of an external server. This was made possible through the magic of the GitLab CI (Continuous Integration) Scheduling.</p>
<p>For those not familiar with <strong>GitLab Pages</strong>, it uses a CI configuration file (<strong>.gitlab-ci.yml</strong>) to define how the website is built from the GitLab project source files. See my <a href="/posts/2018/06/moving-your-jekyll-site-from-github-pages-to-gitlab/">previous post</a>
 which covers this process in detail. To summarize, each time the site source files are updated, <strong>GitLab</strong> starts a CI Pipeline and rebuilds the website following the directions in the CI configuration file. As I learned more about the GitLab CI process, I discovered CI Pipelines can be scheduled to run automatically.</p>
<p>This site uses the <strong>Jekyll</strong> <code>sample</code> filter to select a random quote for the main page <em>Message of the Day</em>, which requires only a simple site <em>rebuild</em> to update the message. For the full details on this process, see my original post <a href="https://bftsystems.ca/automating-updates-to-github-pages/" target="_blank">here</a>
. Using the GitLab CI Scheduler, I could schedule a <em>rebuild</em> at midnight each day to update the site, which would eliminate all external server dependencies.</p>
<p>To add a <em>Pipeline Schedule</em> to a project in GitLab, open the project and select <strong>CI / CD</strong>, <strong>Schedules</strong>. All active and inactive schedules will appear on this page as shown below.</p>

    <img src="gitlab_ci_schedules.png"  alt="GitLab CI Schedules"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 90%;"  />


<p>To add a new schedule, click on the <strong>New Schedule</strong> button and fill in the details as necessary, such as the <em>Description</em>, <em>Interval</em>, <em>Timezone</em> and <em>Target Branch</em>. There are several default intervals available, and custom intervals can be entered using standard Unix <em>cron</em> format. This schedule shown below will force a <em>rebuild</em> of the project each day at 12:01am, no external server needed.</p>

    <img src="gitlab_ci_schedule.png"  alt="GitLab CI Schedule"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 90%;"  />


<p>There are many more advanced options available through <em>GitLab CI</em>, for example, variables can be included in the schedule, which can be referenced in the <em>CI configuration</em> file allowing further customizations of the build process. For more information on this process, refer to the <a href="https://docs.gitlab.com/ee/api/pipeline_schedules.html" target="_blank">GitLab Pipeline schedules documentation</a>
. I&rsquo;m already looking into other ways to customize this site using variables in the <strong>GitLab CI</strong> build process. Watch for future updates here.</p>
<p>To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
. Thanks for reading.</p>
]]></content></item><item><title>Create Twitterbot API Access Tokens using Twurl</title><link>/posts/2018/08/create-twitterbot-api-access-tokens-using-twurl/</link><pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate><guid>/posts/2018/08/create-twitterbot-api-access-tokens-using-twurl/</guid><description>If you do a quick Google search for &amp;ldquo;twitterbot&amp;rdquo;, you&amp;rsquo;ll find countless articles outlining the steps to setup a Twitterbot account and create the necessary Twitter API keys using this account. Unfortunately, with the recent Twitter Developer platform changes, they have significantly restricted the Developer approval process. In this new environment, it makes little sense to setup a separate Developer account for the purposes of a single Twitterbot (and in all likelihood, the request may be denied by Twitter anyway).</description><content type="html"><![CDATA[<p>If you do a quick Google search for <strong>&ldquo;twitterbot&rdquo;</strong>, you&rsquo;ll find countless articles outlining the steps to setup a Twitterbot account and create the necessary Twitter API keys using this account. Unfortunately, with the recent Twitter Developer platform changes, they have significantly restricted the Developer approval process. In this new environment, it makes little sense to setup a separate Developer account for the purposes of a single Twitterbot (and in all likelihood, the request may be denied by Twitter anyway). This is where the <strong>twurl</strong> utility comes in.</p>
<p><a href="https://github.com/twitter/twurl" target="_blank">Twurl</a>
 is a Ruby utility similar to the Unix tool <strong>curl</strong>, but designed specifically for  the Twitter API. The primary purpose of this tool is for testing and debugging Twitter API calls, but it can also be used to generate Twitter User <strong>Access Tokens</strong> for Twitter test accounts, or in this case, for a Twitterbot. <strong>Twurl</strong> can be easily installed using Ruby Gems with this command (assuming you already have Ruby setup on your system).</p>
<pre><code>gem install twurl
</code></pre>
<p>Once <strong>twurl</strong> is installed, follow the usual steps to create a new Twitter Application ID in the <a href="https://developer.twitter.com/" target="_blank">Twitter Developer Portal</a>
 using your primary Twitter Developer account. This process is covered in my previous <a href="https://bftsystems.ca/twitter-developer-changes/" target="_blank">post</a>
 regarding the Twitter Developer Platform changes. In addition to this, you will need to setup a separate Twitter account for your bot.</p>
<p>Once you have the Twitter App consumer (API) key and secret you must authorize <strong>twurl</strong> to make API requests using them. Open the <code>terminal</code> and enter the following command (inserting your Twitter APP api key and secret where appropriate).</p>
<pre><code>twurl authorize --consumer-key API_KEY --consumer-secret API_SECRET
</code></pre>
<p><strong>Twurl</strong> will return a URL on the command line and prompt you to enter a <strong>PIN</strong> number. Cut and paste this URL into your browser and authenticate to Twitter using your Twitterbot account username and password. You should then be prompted with the usual Twitter app authorization page.</p>

    <img src="twitter_authorize_app.png"  alt="Twitter Authorize App"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%"  />


<p>Once you click <strong>Authorize app</strong>, Twitter will return a <strong>PIN</strong> number</p>

    <img src="twitter_pin.png"  alt="Twitter App Pin"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>Enter this <strong>PIN</strong> number in the terminal where prompted by <strong>Twurl</strong>. Assuming there are no issues, <strong>twurl</strong> should report <code>Authorization successful</code>. Once this is done, your new <em>Consumer (API) Key</em>, <em>Consumer (API) Secret</em>, <em>Access Token</em> and <em>Access Token Secret</em> will be stored in the <code>.twurlrc</code> file. This file can be viewed in any text editor. For example, on my system running <strong>macOS</strong>, I opened this file using <code>nano ~/.twurlrc</code> and it looks as follows (keys blanked out).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">profiles</span>:
  <span style="color:#f92672">BFTbot</span>:
    <span style="color:#f92672">5E7GtcDZdcqYWjMxjhdzeZqF1</span>:
      <span style="color:#f92672">username</span>: <span style="color:#ae81ff">BFTbot</span>
      <span style="color:#f92672">consumer_key</span>: <span style="color:#ae81ff">XXXXXXXXXXXXXXXXXXXXXXXXX</span>
      <span style="color:#f92672">consumer_secret</span>: <span style="color:#ae81ff">XXXXXXXXXXXXXXXXXXXX-XXXXXXXXXXXXXXXXXXXXXXXX</span>
      <span style="color:#f92672">token</span>: <span style="color:#ae81ff">XXXXXXXXXXXXXXXXXXXXXXXXX</span>
      <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</span>
<span style="color:#f92672">configuration</span>:
  <span style="color:#f92672">default_profile</span>:
  - <span style="color:#ae81ff">BFTbot</span>
  - <span style="color:#ae81ff">5E7GtcDZdcqYWjMxjhdzeZqF1</span>
</code></pre></div><p>You can the cut and paste the <em>consumer_key</em>, <em>consumer_secret</em>, <em>token</em> and <em>secret</em> from this file into your bot script and run it following the usual process (see my previous Twitterbot <a href="http://localhost:4000/twitter-motd-bot/" target="_blank">post</a>
  for details). Your bot will access the Twitter API and authenticate using your Twitterbot account, with no developer credentials needed.</p>
<p>One item to note, Twitter limits each developer account to a maximum of 10 apps to <em>combat spam and multi-key abuse</em>. If you wish to create more than 10 apps, you will need to follow the steps <a href="https://developer.twitter.com/en/docs/basics/developer-portal/guides/apps" target="_blank">here</a>
 to submit a request to Twitter for additional app approvals. Lastly, as any Twitterbots you create are now associated with APP IDs under your primary Twitter account, you will want to ensure they don&rsquo;t break any of Twitter&rsquo;s rules to ensure your account stays in good standing. Refer to the Twitter Developer <a href="https://developer.twitter.com/en/docs/basics/security-best-practices" target="_blank">Security Best Practices</a>
 page for more information.</p>
<p>To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>The Mac Pro is dead, long live the Mac Mini?</title><link>/posts/2018/08/the-mac-pro-is-dead-long-live-the-mac-mini/</link><pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate><guid>/posts/2018/08/the-mac-pro-is-dead-long-live-the-mac-mini/</guid><description>I try to avoid opinion pieces, but with the rumours this week that Apple is reportedly updating the Mac Mini this fall , I needed to get my thoughts on the subject off my chest.
I think it is has been clear for a while that Apple has not been paying close attention to the Mac lineup. For example, in the recent Apple quarterly earnings report, Mac sales showed a 13% year-over-year decline.</description><content type="html"><![CDATA[<p>I try to avoid opinion pieces, but with the rumours this week that <a href="https://www.macworld.com/article/3289149/macs/apple-mac-mini-rumors.html" target="_blank">Apple is reportedly updating the Mac Mini this fall</a>
, I needed to get my thoughts on the subject off my chest.</p>

    <img src="mac_mini_2012.jpg"  alt="Mac Mini 2012"  class="center"  style="border-radius: 4px; width: 75%;"  />


<p>I think it is has been clear for a while that Apple has not been paying close attention to the Mac lineup. For example, in the recent Apple quarterly earnings report, Mac sales showed a 13% year-over-year decline. Although there may be other reasons for this drop, it&rsquo;s not the sign of a healthy product portfolio. Don&rsquo;t forget that last year Apple executives openly admitted the current <a href="https://www.theverge.com/2017/4/4/15175994/apple-mac-pro-failure-admission" target="_blank">Mac Pro is a failure</a>
 and they plan for a revamped version in 2019.</p>
<p>That is great news for many, but why is it taking so long to replace the Mac Pro? Firstly, it is clear that Apple leadership gives <em>Product design</em> priority, and all their products must be <a href="https://en.wikipedia.org/wiki/Objet_d%27art" target="_blank">Objet d&#39;art</a>
 irregardless of the use case. As someone who uses Apple products on a daily basis, I appreciate this attention to detail. That said, perhaps a little more focus could be spent on delivering a product which meets the needs of the intended customer. Who has not heard the phrase &ldquo;Perfection is the enemy of done&rdquo;? I think this could explain some of the reasons behind Apple&rsquo;s current Mac line follies.</p>
<p>In the case of the Mac Mini and Pro, I believe by and large both these products now cater to same customers. How many of us have an iMac, MacBook Pro or Mac Pro on our desks, and also a Mac Mini on the workbench, in the living room, or in a closet somewhere? In past, the Mac Mini was developed by Apple as a <em>gateway</em> product which helped bring new customers into their ecosystem. In the current market, that is no longer the case, as many people now primarily use laptops or tablets and will likely never own a desktop PC in their lifetimes.</p>
<p>So, if the Mini and Pro cater to the same or similar markets, why have two distinct products at all? This is the question I have been asking myself recently. For years I owned a 2012 Mac Mini, which I also upgraded several times during its life. I could do this because Apple DESIGNED the product to be easily opened and upgraded (although the hard drive was a little persnickety to swap). Knowing this, why couldn&rsquo;t a new Mac Mini with easily upgradeable memory and storage also be the <em>&ldquo;core&rdquo;</em> of a new modular Mac Pro product line?</p>
<p>Looking at the current Mac Pro (the &ldquo;Trashcan&rdquo; Mac), the most significant issue was the GPUs, which were dated within months of release, and no upgrade path was available. This problem could be resolved by including expansion slots in a <em>new</em> Mac Pro, but somehow I think this flies in the face of current Apple <em>ethos</em>. Following my previous logic, this could be handled by a Mac Pro (aka Mini) <em>&ldquo;core&rdquo;</em> system using an external eGPU enclosure (or RAID for additional storage) connected using Thunderbolt or USB 3.1. Apple is already going down this path with the MacBook Pro line, so a desktop system using the same ecosystem of expansion products would make a lot of sense. Yes, this is likely the fantasy of a long-time Mac fan, but it&rsquo;s sure fun to think about.</p>
<p>If you would like to contact me or provide feedback on this post, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>Twitter Developer Platform process changes</title><link>/posts/2018/08/twitter-developer-platform-process-changes/</link><pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate><guid>/posts/2018/08/twitter-developer-platform-process-changes/</guid><description>If you are any kind of Web developer, you have likely heard all about the changes Twitter has made to their Developer Platform. Primarily, the changes impact developers of 3rd party Twitter apps due to the removal of several legacy API endpoints, such as the Tweet streaming and Direct Messaging APIs. These changes have received a lot of coverage, but there has been little said about the impact for small in-house and student application developers.</description><content type="html"><![CDATA[<p>If you are any kind of Web developer, you have likely heard all about the changes Twitter has made to their Developer Platform. Primarily, the changes impact developers of 3rd party Twitter apps due to the removal of several legacy API endpoints, such as the Tweet streaming and Direct Messaging APIs. These changes have received a lot of coverage, but there has been little said about the impact for small in-house and student application developers. In many cases, the primary concern is the requirement to <a href="https://developer.twitter.com/en/apply/user" target="_blank">apply for a Twitter Developer account</a>
 and be approved by Twitter before you can create new apps.</p>

    <img src="twitter_developer_changes.png"  alt="Twitter App Developer Changes"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>The process is simple enough, just click on the link and complete the developer account application form (which now includes an essay question where you must explain how you intend to use the Twitter APIs). How long it takes for an account to be approved seems to vary wildly. For developers with applications already deployed under the old system, the process seems instantaneous. Complete the application, confirm your email address and you are approved. In other cases, especially for students or new developers building their first Twitter application, it can take days to weeks, with little communication provided by Twitter when their application will be approved, other than this status page.</p>

    <img src="twitter_app_under_review.png"  alt="Twitter App Under Review"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>One can only hope this is a temporary issue due to the number of new applications being reviewed by Twitter at the moment.</p>
<p>Once your application has been approved, this gives you access to the new <a href="https://developer.twitter.com/" target="_blank">Twitter Developer Portal</a>
. Twitter has now unified all developer documentation and account management through this new interface, and there is plenty of information to be found there. For any new developers, the most important sections are the <strong>Get Started</strong> and <strong>Apps</strong> menus on the far right of the page.</p>

    <img src="twitter_apps_menu.png"  alt="Twitter Apps Menu"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>The <strong>Get Started</strong> page covers all the basic steps to begin using the Developer Portal, such as adding developers to your account (if working in a team), and setting up Twitter <strong>dev environments</strong>, which are necessary if you will be using the new paid <a href="https://developer.twitter.com/en/pricing" target="_blank">Premium and Enterprise APIs</a>
.</p>
<p>Next, the <strong>Apps</strong> page lists any previously created applications, and new apps can be created by simply clicking on the <strong>Create an app</strong> button.</p>

    <img src="twitter_create_an_app.png"  alt="Twitter Create an App"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>The process for creating new Twitter apps is similar to the previous application process, although under Twitter&rsquo;s new process you <strong>must</strong> include a long-form description how the app will be used.</p>

    <img src="twitter_app_use.png"  alt="Twitter Create App Use"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>Once your app has been successfully created, click on the <strong>Keys and tokens</strong> tab to access the Consumer (API) keys and User Access tokens for your application. At this point, the process to use these keys in your application matches the old Twitter API key process. Just copy the keys into your application and start testing.</p>

    <img src="twitter_api_keys.png"  alt="Twitter API keys"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>In this post, I have only scratched the surface of the new <strong>Twitter Developer Portal</strong> to help you get started building apps. In a future post I will review some of the new features available, such as the <strong>Developer Sandbox Environments</strong> for testing access to the Twitter Premium APIs.</p>
<p>To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>Build a Twitter Message of the Day (MOTD) Bot</title><link>/posts/2018/07/build-a-twitter-message-of-the-day-motd-bot/</link><pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate><guid>/posts/2018/07/build-a-twitter-message-of-the-day-motd-bot/</guid><description>I was first introduced to the concept of the message of the day (MOTD) while working as a SunOS system administrator back in my university days. At our school, the systems used the Unix fortune command to display random messages and quotes on login. Being a bit nostalgic about these early days of the personal computer era, I decided to build a Message of the Day bot for Twitter. This would give me the pleasure of seeing my own custom messages of the day in my feed, and help me learn the ins and outs of the Twitter API.</description><content type="html"><![CDATA[<p>I was first introduced to the concept of the <a href="https://en.wikipedia.org/wiki/Motd_%28Unix%29" target="_blank">message of the day (MOTD)</a>
 while working as a <strong>SunOS</strong> system administrator back in my university days. At our school, the systems used the Unix <code>fortune</code> command to display random messages and quotes on login. Being a bit nostalgic about these early days of the personal computer era, I decided to build a <em>Message of the Day</em> bot for Twitter. This would give me the pleasure of seeing my own custom messages of the day in my feed, and help me learn the ins and outs of the Twitter API. I have plans to develop more advanced Twitter bots in future, so now was as good a time as any to learn the new API.</p>
<p>As with my previous projects, I am running this Twitterbot using Python on one of my VMs running on AWS. I researched several libraries available for accessing Twitter in Python and decided to use the <a href="https://github.com/ryanmcgrath/twython" target="_blank">Twython library</a>
 to handle the API calls. The <strong>Twython</strong> library can be installed using the <strong>PIP</strong> package manager for Python, as shown below. Most newer versions of <strong>Linux</strong> include <strong>PIP</strong> by default, so the first command may not be necessary on some systems, but I included it just in case.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sudo apt<span style="color:#f92672">-</span>get install python<span style="color:#f92672">-</span>pip
sudo pip install twython
</code></pre></div><p>Once <strong>Twython</strong> has been installed successfully, the next step is to create a Twitter account for the bot. Please note, you <strong>must</strong> assign a mobile phone number to this account, at least temporarily. Twitter will not allow apps to be created using an account which does not have a verified mobile phone number.</p>
<p>After creating your new Twitter bot account, go to <a href="https://apps.twitter.com" target="_blank">https://apps.twitter.com/</a>
 and click the <strong>Create New App</strong> button. When prompted, enter the following information:</p>
<ul>
<li>Name for the application</li>
<li>Description</li>
<li>Link to valid website (I used my site homepage)</li>
</ul>
<p>All other fields can be left blank. Check the boxed labelled <em>Yes, I have read and agree to the Twitter Developer Agreement</em> (assuming you read and agree), then click the button labelled <strong>Create your Twitter application</strong>. Once the app has been created, click on the <strong>Keys and Access Tokens</strong> tab to access your new app Consumer (API) keys.</p>

    <img src="Twitter_App_Keys.jpg"  alt="Twitter App keys"  class="left"  style="border-radius: 5px; margin-left: 1em; width: 95%;"  />


<p>You will also need <strong>Access Tokens</strong> to enable API access for your bot account. Click on the <strong>Regenerate Consumer Key and Secret</strong> link to generate these codes. Keep a copy of the <em>Consumer (API) Key</em>, <em>Consumer (API) Secret</em>, <em>Access Token</em> and <em>Access Token Secret</em> to be used later in your bot script. Also note, once this is done you can remove the mobile phone number assigned in <em>Account Settings</em> under the Twitter bot account. It is no longer required to create additional apps.</p>
<p>We&rsquo;re finally at the point where we can build and run our new Twitterbot. Here is a breakdown of the Python code for my <em>Message of the Day</em> bot. The first section includes standard declarations identifying any required Pythong libraries. It then opens a file named <strong>quotes.csv</strong> which is a simple comma delimited file of quotes I have collected over the years. The file contains two columns of information, the first column being the <code>quote</code>, the second column being the <code>citation</code> associated with the quote, as shown below.</p>
<table>
<thead>
<tr>
<th>quote</th>
<th>citation</th>
</tr>
</thead>
<tbody>
<tr>
<td>When opportunity knocks, bolt the door.</td>
<td>Grumpy Cat</td>
</tr>
<tr>
<td>The unexamined life is not worth living.</td>
<td>Socrates</td>
</tr>
</tbody>
</table>
<p>The file is read directly into a Python <em>dictionary</em> array for simple access in memory. Unfortunately, this includes the CSV file <em>header</em> row, which the last line of code deletes from the array to ensure it is not selected randomly in error.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#!/usr/bin/python</span>
<span style="color:#f92672">import</span> csv<span style="color:#f92672">,</span> random<span style="color:#f92672">,</span> sys
<span style="color:#f92672">from</span> twython <span style="color:#f92672">import</span> Twython, TwythonError

<span style="color:#75715e"># Import quotes dictionary from csv file</span>
quotes <span style="color:#f92672">=</span> dict(csv<span style="color:#f92672">.</span>reader(open(<span style="color:#e6db74">&#34;/home/pi/etc/quotes.csv&#34;</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>)))
<span style="color:#75715e"># Remove CSV header row from dictionary to avoid random selection</span>
<span style="color:#66d9ef">del</span> quotes[<span style="color:#e6db74">&#39;quote&#39;</span>]
</code></pre></div><p>The next section of code uses the Python <code>random.choice</code> function to select a random quote from the <em>quotes</em> array. The quote and citation are assembled in the variable <code>motd</code> using <code>utf-8</code> encoding which allows the <em>em dash</em> character to be included as part of the citation. Lastly, some quotes in the <strong>quotes.csv</strong> source file may be longer than the maximum allowed Tweet size (280 characters). To avoid this issue, the script uses a selection loop that repeats until a <em>Message of the Day (motd)</em> is selected which is less than 278 characters (just to play it safe).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Select random quote</span>
<span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
	quote <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(quotes<span style="color:#f92672">.</span>keys())
	cite <span style="color:#f92672">=</span> quotes[quote]
	motd <span style="color:#f92672">=</span> <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> quote<span style="color:#f92672">.</span>decode(<span style="color:#e6db74">&#39;utf8&#39;</span>) <span style="color:#f92672">+</span> <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\&#34;\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34; </span><span style="color:#ae81ff">\u2014</span><span style="color:#e6db74"> &#34;</span> <span style="color:#f92672">+</span> cite<span style="color:#f92672">.</span>decode(<span style="color:#e6db74">&#39;utf8&#39;</span>) <span style="color:#f92672">+</span> <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;  #motd&#34;</span>

	<span style="color:#75715e"># if less than max tweet size, break out of loop and tweet</span>
	<span style="color:#66d9ef">if</span> len(motd) <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">278</span>:
		<span style="color:#66d9ef">break</span>
</code></pre></div><p>Once a quote has been selected and properly formatted, the script uses the <strong>Twython</strong> library to submit the Tweet via the Twitter API. For this to function, the <em>Consumer (API) Key</em>, <em>Consumer (API) Secret</em>, <em>Access Token</em> and <em>Access Token Secret</em> generated previously need to be inserted into the appropriate variables in the code below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Twitter authentication settings. Create a Twitter app at https://apps.twitter.com/ and</span>
<span style="color:#75715e"># generate key, secret, etc, and insert them below.</span>
apiKey <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
apiSecret <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
accessToken <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
accessTokenSecret <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>

<span style="color:#75715e"># Tweet Message of the Day</span>
<span style="color:#66d9ef">try</span>:
	twitter_api <span style="color:#f92672">=</span> Twython(apiKey,apiSecret,accessToken,accessTokenSecret)
	twitter_api<span style="color:#f92672">.</span>update_status(status <span style="color:#f92672">=</span> motd)
	print motd
<span style="color:#66d9ef">except</span> TwythonError <span style="color:#66d9ef">as</span> e:
	print(e)
</code></pre></div><p>On my server, I created this script with the filename <code>motd_bot.py</code>. I also enabled the <em>executable bit</em> on this file with the command <code>chmod +x motd_bot.py</code>. This allows running the script directly from the command line without calling Python first. After a few manual test runs to ensure it was working properly, I used the <code>crontab -e</code> command and added the following lines in the <code>crontab</code> file. This scheduled the script to run daily at 7:00am.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Tweet random MOTD at 7:00am</span>
<span style="color:#ae81ff">0</span> <span style="color:#ae81ff">7</span> * * * /home/bft/bin/motd_bot.py
</code></pre></div><p>One last item to note, as this script includes your Twitter API keys and secrets, it should only be stored and run from a properly secured system. In my example, the script is running on an Ubuntu Server VM which has been hardened following some of the steps from this <a href="https://gist.github.com/lokhman/cc716d2e2d373dd696b2d9264c0287a3" target="_blank">page</a>
 on GitHub. Refer to the Twitter Developer <a href="https://developer.twitter.com/en/docs/basics/security-best-practices.html" target="_blank">Security Best Practices</a>
 page for more information.</p>
<p>As always, I hope you found this post useful, or at least entertaining. To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>What Happened to Windows Update?</title><link>/posts/2018/07/what-happened-to-windows-update/</link><pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate><guid>/posts/2018/07/what-happened-to-windows-update/</guid><description>Recently, my wife mentioned she was seeing some warning on her computer that it was almost out of disk space. I thought this odd as her system has a 500GB SSD, and last I checked it still had over 400GB free. To play it safe, I took a quick look at her system and found it had a new (and unknown) drive labeled D: with just under 50MB available. This was the drive causing the Low Disk Space warning.</description><content type="html"><![CDATA[<p>Recently, my wife mentioned she was seeing some warning on her computer that it was almost out of disk space. I thought this odd as her system has a 500GB SSD, and last I checked it still had over 400GB free. To play it safe, I took a quick look at her system and found it had a new (and unknown) drive labeled D: with just under 50MB available. This was the drive causing the <em>Low Disk Space</em> warning.</p>

    <img src="Low_Disk_Space.png"  alt="Low Disk Space"  class="left"  style="border-radius: 4px; margin-left: 1em;"  />


<p>Of course, my first thought was the system had some form of malware on it (it is a Windows system after all). Upon further investigation, I discovered this was the <em>OEM partition</em>, which is normally hidden, but for whatever reason it now had a drive letter assigned. After some research I discovered this issue was caused by a recent update from Microsoft<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Unfortunately, as this is a protected partition, there is no way to remove the drive letter and hide the partition using the Windows <strong>Disk Management</strong> tool.</p>

    <img src="Disk_Management.png"  alt="Disk Management"  class="left"  style="border-radius: 4px; margin-left: 1em; width: 95%;"  />


<p>I found several posts on the <em>Microsoft Answers</em> forums which described a complex, multistep process using <code>diskpart</code> to remove the drive letter. I dug further and found the simplest (and safest) solution was to open <strong>Command Prompt</strong> as <em>Administrator</em>, and run the <code>mountvol D: /d</code> command to remove the drive letter and re-hide the partition. This would finally stop the bogus <em>Low Disk Space</em> warnings caused by the bad Windows April 2018 Update.</p>
<p>Once that was fixed, I decided to run <strong>Windows Update</strong> to check for new updates. The system reported it had checked for updates late yesterday and was <em>up to date</em> (at least that&rsquo;s what it reported).</p>

    <img src="Windows_Update.png"  alt="Windows Update"  class="left"  style="border-radius: 4px; margin-left: 1em;"  />


<p>However, I decided to click the <strong>Check for updates</strong> button anyway, and was surprised when the system reported new updates were available (some almost a month old) and was now downloading and installing the updates.</p>
<p>So, what has happened at <strong>Microsoft</strong> that the software update process has gotten this bad? I&rsquo;ve lost count the number of times in recent months I have needed to fix a family member or client’s computer due to a bad update from Microsoft. Years ago, before the Windows 8/10 debacle, I recall a time when Windows Update generally “just worked”. I could setup a PC for a client or friend, enable Windows Update, and just forgot about it. After that, the only time I heard from them was when their system had been infected with some form of malware (we’re talking Windows PCs after all). Now it seems each month on <em>Patch Tuesday</em> it has become a <em>crap shoot</em> where you have no idea what <strong>Microsoft</strong> is going to break next.</p>
<p>In my opinion, this is an untenable situation where <strong>Microsoft</strong> is actively increasing risk for the entire industry. The poor handling<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> and quality assurance of recent updates is causing both businesses and home users to <em>shy away</em> from installing them. Many large enterprises were always somewhat reticent to install updates, but now I’m even seeing this in my smaller business clients who, after being burned by one too many bad updates, suggest I wait <em>a month or two</em> before installing the new updates. This only increases risk for everybody, as the most effective security practice available is to ensure that systems are kept <strong>up to date</strong> with software patches. I don’t know what the solution is to this issue, other than for significant changes at Microsoft. As users, all we can do is keep pointing out the impact of these mistakes and hope someone eventually listens and takes action.</p>
<p>As always, I hope you found this post useful, or at least entertaining. To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://answers.microsoft.com/en-us/windows/forum/windows_10-files/new-partitions-may-appear-in-file-explorer-after/115d2860-542e-410f-983c-2aeb8bbd7d13" target="_blank">New partitions may appear in File Explorer after installing the Windows 10 April 2018 Update (version 1803)</a>
&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://www.extremetech.com/computing/212724-microsoft-kills-patch-notes-will-no-longer-explain-most-windows-10-updates" target="_blank">Microsoft kills patch notes, will no longer explain most Windows 10 updates</a>
&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item><item><title>Using PowerShell to Find Inactive Users in AD and Exchange</title><link>/posts/2018/07/using-powershell-to-find-inactive-users-in-ad-and-exchange/</link><pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate><guid>/posts/2018/07/using-powershell-to-find-inactive-users-in-ad-and-exchange/</guid><description>Recently, I was working with a client to consolidate of several Active Directory and LDAP domains into a single unified domain. To ensure we were starting with a clean slate, one of the first steps I performed was an audit to find any inactive accounts in the Active Directory domains. This allowed the client to investigate and/or remove any stale accounts prior to consolidation. There are many professional (and possibly expensive) tools which can be used to automate this process, but for small to mid-sized domains, this can be done fairly easily using the following PowerShell command and some Excel manipulation.</description><content type="html"><![CDATA[<p>Recently, I was working with a client to consolidate of several Active Directory and LDAP domains into a single unified domain. To ensure we were starting with a <em>clean slate</em>, one of the first steps I performed was an audit to find any inactive accounts in the Active Directory domains. This allowed the client to investigate and/or remove any stale accounts prior to consolidation. There are many professional (and possibly expensive) tools which can be used to automate this process, but for small to mid-sized domains, this can be done fairly easily using the following <strong>PowerShell</strong> command and some <strong>Excel</strong> manipulation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">Get-AdUser -SearchBase <span style="color:#e6db74">&#34;ou=users,dc=company,dc=com&#34;</span> -Filter * -Properties * | select SamAccountName, distinguishedName, LastLogonDate, Department, Title, Enabled | Export-Csv C:\Temp\AdUserLastLogonDate.csv
</code></pre></div><p>In this example, I have focused on the the <strong>users</strong> OU, although this script could be used to investigate stale computer or service accounts as well. While I only needed the user&rsquo;s account name and <em>LastLogonDate</em> in this report, I also included fields such as <em>Department</em> and <em>Title</em> to assist in identifying the users. The CSV file produced by <strong>PowerShell</strong> could then be opened in <strong>Excel</strong> to quickly locate any inactive accounts by sorting on the <em>LastLogonDate</em> field. That said, there are some specific issues to be aware of using this process.</p>
<ol>
<li>The <em>LastLogonDate</em> attribute can be out by up to 14 days (due to how this field is replicated in AD).</li>
<li>If the <em>LastLogonDate</em> is blank, this indicates the account has never logged in.</li>
</ol>
<p>With the information provided in this report, I was able to work with the client to identify stale accounts which could be removed or further investigated. In at least one case, using this report the client identified a service which had not been working in months and was only brought to their attention when the service account showed up as <em>inactive</em>.</p>
<p>Once this was done, the client decided to perform a similar audit of user mailboxes in Exchange. As above, this can be done fairly easily with a few <strong>PowerShell</strong> commands and <strong>Excel</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">$UserCredential = Get-Credential
$Session = New-PSSession -ConfigurationName Microsoft.Exchange -ConnectionUri https<span style="color:#960050;background-color:#1e0010">:</span>//outlook.office365.com/powershell-liveid/ -Credential $UserCredential -Authentication Basic -AllowRedirection
Get-mailbox -OrganizationalUnit <span style="color:#e6db74">&#34;ou=users,dc=company,dc=com&#34;</span> -resultsize unlimited | Get-MailboxStatistics | select DisplayName, TotalItemSize, Itemcount, legacyDN, lastlogontime | Export-Csv C:\Temp\ExchangeMBOXReport.csv
</code></pre></div><p>In this example, the client mailboxes are hosted on <strong>Office 365</strong>. The first two commands are required to authenticate and connect to the <strong>Exchange Online PowerShell</strong> interface. Once connected, the <code>Get-Mailbox</code> command is used to export all user mailboxes, and filtered through the <code>Get-MailboxStatistics</code> command, giving the client a quick and dirty report of mailbox usage as well. Once again, the CSV file produced by <strong>PowerShell</strong> could be opened and manipulated in <strong>Excel</strong> to quickly locate any inactive mailboxes by sorting on the <em>lastlogontime</em> field.</p>
<p>Unfortunately, in more complex environments where <strong>Blackberry BES</strong> (yes, that is still being used out there) or some other Server-Side Antivirus tools, the <em>lastlogontime</em> field may not be correct, as this date is updated every time the mailbox is scanned. In these cases, you will need to use a different process to identify <em>inactive</em> mailboxes. One possible way is to report based on the date of the last item in the <em>Sent Items</em> folder. Rather than get into the details here, I&rsquo;ll refer you to this <a href="https://gallery.technet.microsoft.com/scriptcenter/List-Inactive-Mailboxes-on-1ac82ddf" target="_blank">article</a>
 on the <strong>Microsoft Script Center</strong> which covers the process in depth.</p>
<p>As always, I hope you found this post useful, or at least interesting. To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>Simple script to monitor for website changes</title><link>/posts/2018/06/simple-script-to-monitor-for-website-changes/</link><pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate><guid>/posts/2018/06/simple-script-to-monitor-for-website-changes/</guid><description>As I may have mentioned in previous posts, I run several small websites, both professionally and for personal interests. The majority are simple static websites, or low volume blogs which only expect a couple of updates in a week. With the various issues recently of crypto-miners and other exploits, I wanted a process in place to monitor for site issues, especially if the site changed unexpectedly. Of course, I could setup a trigger using IFTTT to monitor the sites and notify me in the event of changes, but as always, I wanted a “home-grown” solution I could customize.</description><content type="html"><![CDATA[<p>As I may have mentioned in previous posts, I run several small websites, both professionally and for personal interests. The majority are simple static websites, or low volume blogs which only expect a couple of updates in a week. With the various issues recently of crypto-miners and other exploits, I wanted a process in place to monitor for site issues, especially if the site changed unexpectedly. Of course, I could setup a trigger using <a href="https://ifttt.com" target="_blank">IFTTT</a>
 to monitor the sites and notify me in the event of changes, but as always, I wanted a “home-grown” solution I could customize. As I already have several VMs in production performing various chores for me, I decided these would be perfect for the job.</p>
<p>For the sake of simplicity, I wrote a simple <strong>bash</strong> script which downloads the site home page using <strong>curl</strong> and compares if the page has changed from the previous run. This has the added benefit if the site is down, an error will be generated also causing an alert. If an issue is identified, the script sends an email notifying me to investigate.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span>                                                                                                                            
<span style="color:#75715e"># Monitors a website for changes and sends notification</span>

DIRECTORY<span style="color:#f92672">=</span><span style="color:#e6db74">`</span>dirname $0<span style="color:#e6db74">`</span>
CURRENT<span style="color:#f92672">=</span>$DIRECTORY/current.html
PREVIOUS<span style="color:#f92672">=</span>$DIRECTORY/previous.html
URL<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://thedeskofbrad.ca&#34;</span>

mv $CURRENT $PREVIOUS 2&gt; /dev/null
curl $URL -L --compressed -s &gt; $CURRENT
DIFF_OUTPUT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#66d9ef">$(</span>diff $CURRENT $PREVIOUS<span style="color:#66d9ef">)</span><span style="color:#e6db74">&#34;</span>
<span style="color:#66d9ef">if</span> <span style="color:#f92672">[</span> <span style="color:#e6db74">&#34;0&#34;</span> !<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${#</span>DIFF_OUTPUT<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">]</span>; <span style="color:#66d9ef">then</span>
  echo <span style="color:#e6db74">&#34;Site https://thedeskofbrad.ca has changed&#34;</span> | mail -s <span style="color:#e6db74">&#34;Site update report&#34;</span> email@domain.com
<span style="color:#66d9ef">fi</span>
</code></pre></div><p>As most of the sites being monitored are low volume sites, I decided to run this script every 8 hours using <strong>cron</strong>. To do this, I used the command <code>crontab -e</code> and added the following line for this purpose.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># m h  dom mon dow   command</span>
<span style="color:#ae81ff">0</span> */8 * * * /home/pi/bin/thedeskofbrad/monitor.sh
</code></pre></div><p>With this setup, if the site goes down or is compromised somehow, I will be notified by email within 8 hours at most. Depending on the importance of the site, I could decrease the interval to as low as once per minute, but 8 hours seemed enough for my purposes.</p>
<p>As this is a simple script which I developed specifically for my purposes, it has some obvious caveats. For example, if a page other than the home page is changed, it will not be reported. Also, if in future I add dynamic content such as <strong>Google Ads</strong>, this would break functionality, as the ads will change on each page request. If you decide to use this script for yourself, take these issues into consideration.</p>
<p>As always, I hope you found this post useful, or at least interesting. To contact me, please use the <a href="/contact">Contact</a>
 page, or message me on <a href="https://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks for reading.</p>
]]></content></item><item><title>Moving your Jekyll site from GitHub Pages to GitLab</title><link>/posts/2018/06/moving-your-jekyll-site-from-github-pages-to-gitlab/</link><pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate><guid>/posts/2018/06/moving-your-jekyll-site-from-github-pages-to-gitlab/</guid><description>For the last several months, I&amp;rsquo;ve been pondering the move of this site from GitHub Pages to GitLab. I have several reasons which I won&amp;rsquo;t get into here, but the recent news regarding the acquisition of GitHub by Microsoft gave me the impetus to get this done. I ran into several interesting &amp;ldquo;hiccups&amp;rdquo; during the process, and decided to write this post to help anyone else going down this road.</description><content type="html"><![CDATA[<p>For the last several months, I&rsquo;ve been pondering the move of this site from GitHub Pages to GitLab. I have several reasons which I won&rsquo;t get into here, but the recent <a href="https://www.bloomberg.com/news/articles/2018-06-03/microsoft-is-said-to-have-agreed-to-acquire-coding-site-github" target="_blank">news</a>
 regarding the acquisition of GitHub by Microsoft gave me the impetus to get this done. I ran into several interesting &ldquo;hiccups&rdquo; during the process, and decided to write this post to help anyone else going down this road.</p>
<p>Obviously, the first step is to migrate your site source files from GitHub to GitLab. In my case, I have a local copy of this site <strong>cloned</strong> on my local system, and I used this as the source. For other options I recommend you read the GitLab document <a href="https://docs.gitlab.com/ee/user/project/import/github.html" target="_blank">Import your project from GitHub to GitLab</a>
. To migrate this site, my first step was to create a new <strong>empty</strong> project on GitLab named <code>bftsystems.gitlab.io</code>. I then cloned this empty repository to my local system, copied my site source files into the empty project, then pushed the results back to GitLab. Here is an overview of the commands I used locally.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd git

git clone git@gitlab.com:bftsystems/bftsystems.gitlab.io.git

cp -R bftsystems.github.io/* bftsystems.gitlab.io/

cd bftsystems.gitlab.io

git add --all

git commit -am <span style="color:#e6db74">&#34;Initial commit to Gitlab hosting&#34;</span>

git push origin master
</code></pre></div><p>Once your repository has been moved to GitLab, nothing will work immediately and no pages will be built. Unlike GitHub, GitLab uses a CI (continuous integration) configuration file to instruct the GitLab site <em>Runners</em> how to build your site. This process is a little more complex than GitHub, but also more flexible. Primarily, this flexibility is what brought me to GitLab for hosting.</p>
<p>To setup the build process, you must create a file named <code>.gitlab-ci.yml</code> in your repository. The contents of this file will direct GitLab how to build your site. The full documentation on this process can be found in the document <a href="https://about.gitlab.com/2016/04/07/gitlab-pages-setup/" target="_blank">Hosting on GitLab.com with GitHub Pages</a>
. As this site is built using Jekyll, I followed the steps in the document outlining the configuration process specifically for Jekyll.</p>
<p>This is where things get a little interesting. When it comes to building the CI file for Jekyll, the GitLab documentation is somewhat vague, and misses several edge cases. Sadly, I seemed to have run into many of those edge cases. Firstly, you must configure your site to use the correct version of Jekyll. This will not be obvious, as the errors provided by GitLab may be non-intuitive. In my case, the errors I received were related to <code>Liquid syntax errors</code> in several source files. After some digging, I discovered these issues were caused by a Jekyll version mismatch. To resolve this, I confirmed the version of Jekyll I&rsquo;m using locally (<code>bundle exec jekyll --version</code>), and updated the <code>Gemfile</code> in my repository to use this version (more on this later).</p>
<p>Further to the Jekyll issue, you must also ensure any Ruby <em>gems</em> (aka <em>plugins</em>) required by your site are added to your site <code>Gemfile</code> as dependencies. Many dependencies included in GitHub by default are not included by GitLab, which will cause build issues. I also believe this is an issue for many other Jekyll templates which assume they are being hosted on GitHub Pages and will encounter dependency issues when hosted on GitLab. To confirm which <em>gems</em> are required, open your site <code>_config.yml</code> file, and locate the section labelled <code>gems:</code> or <code>plugins:</code> (depending on the Jekyll version). Here is the section from this site as an example.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># Plugins (previously gems:)</span>
<span style="color:#f92672">plugins</span>:
  - <span style="color:#ae81ff">jekyll-paginate</span>
  - <span style="color:#ae81ff">jekyll-sitemap</span>
  - <span style="color:#ae81ff">jekyll-gist</span>
  - <span style="color:#ae81ff">jekyll-feed</span>
  - <span style="color:#ae81ff">jemoji</span>
</code></pre></div><p>Using this information and the Jekyll version I confirmed previously, I updated the <code>Gemfile</code> for my site accordingly<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">source &#34;https://rubygems.org&#34;</span>

<span style="color:#ae81ff">gem &#39;jekyll&#39;, &#39;3.6.2&#39;</span>
<span style="color:#ae81ff">gem &#39;jekyll-paginate&#39;</span>
<span style="color:#ae81ff">gem &#39;jekyll-sitemap&#39;</span>
<span style="color:#ae81ff">gem &#39;jekyll-gist&#39;</span>
<span style="color:#ae81ff">gem &#39;jekyll-feed&#39;</span>
<span style="color:#ae81ff">gem &#39;jemoji&#39;</span>
</code></pre></div><p>Interestingly, once these changes were made, I now received page build errors from GitLab regarding <strong>Invalid US-ASCII</strong> characters in several of the CSS files. After further digging through the CSS source files and searching on-line, I discovered GitLab does not support <code>UTF-8</code> by default in their Docker instances. A setting was required in the CI file to enforce this on every build (thanks to <a href="https://gitlab.com/pages/jekyll/issues/9" target="_blank">this</a>
 Jekyll issue report for the discovery). Once this additional configuration was added to the CI file, the site built successfully on GitLab (woot!). Here is the final version of the <code>.gitlab-ci.yml</code> file with all the necessary changes for your reference.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># requiring the environment of Ruby 2.3.x</span>
<span style="color:#f92672">image</span>: <span style="color:#ae81ff">ruby:2.3</span>

<span style="color:#75715e"># add bundle cache to &#39;vendor&#39; for speeding up builds</span>
<span style="color:#f92672">cache</span>:
  <span style="color:#f92672">paths</span>: 
    - <span style="color:#ae81ff">vendor/</span>

<span style="color:#f92672">before_script</span>:
  - <span style="color:#ae81ff">bundle install --path vendor</span>

<span style="color:#75715e"># the &#39;pages&#39; job will deploy and build your site to the &#39;public&#39; path</span>
<span style="color:#f92672">pages</span>:
  <span style="color:#f92672">stage</span>: <span style="color:#ae81ff">deploy</span>
  <span style="color:#f92672">script</span>:
    - <span style="color:#ae81ff">bundle exec jekyll build -d public/</span>
  <span style="color:#f92672">variables</span>:
    <span style="color:#f92672">JEKYLL_ENV</span>: <span style="color:#ae81ff">production</span>
    <span style="color:#f92672">LANG</span>: <span style="color:#e6db74">&#34;C.UTF-8&#34;</span>
  <span style="color:#f92672">artifacts</span>:
    <span style="color:#f92672">paths</span>:
      - <span style="color:#ae81ff">public</span>
  <span style="color:#f92672">only</span>:
    - <span style="color:#ae81ff">master</span> <span style="color:#75715e"># this job will affect only the &#39;master&#39; branch</span>
</code></pre></div><p>Once the build was finished, the site was immediately available through the GitLab Pages subdomain (eg. <code>https://bftsystems.gitlab.io/</code>). The final step in the process was to update DNS for my domain to use GitLab Pages. Once again, this was a little more complex than with GitHub, as there is no automated <strong>https</strong> integration using Let&rsquo;s Encrypt (which GitHub has added recently). I won&rsquo;t go into the specific details of the process here, as it is well documented in the following articles.</p>
<ul>
<li><a href="https://about.gitlab.com/2016/04/07/gitlab-pages-setup/#custom-domains" target="_blank">Hosting on GitLab.com with GitLab Pages</a>
 - <strong>Step 4: Add your custom domain</strong> covers the steps to update your DNS records for GitLab Pages</li>
<li><a href="https://about.gitlab.com/2016/04/11/tutorial-securing-your-gitlab-pages-with-tls-and-letsencrypt/" target="_blank">Securing your GitLab Pages with TLS and Let&amp;rsquo;s Encrypt</a>
</li>
<li><a href="https://about.gitlab.com/2017/02/07/setting-up-gitlab-pages-with-cloudflare-certificates/" target="_blank">Setting up GitLab Pages with CloudFlare Certificates</a>
</li>
</ul>
<p>In the end, I added two <code>A</code> records for my domain, one for the <em>apex</em> or <em>root</em> domain, and a second for the <em>www</em> subdomain. I could have used a single <em>wildcard</em> record, but I try to avoid using wildcard DNS records (just a personal preference). I then added both these domains, with their associated SSL certificates under <strong>Settings, Pages</strong>. I found both domains were required to avoid any SSL warnings when using some browsers.</p>

    <img src="GitLab_Pages_Settings.jpg"  alt="GitLab_Pages_Settings"  class="left"  style="border-radius: 4px; margin-left: 1em;"  />


<p>As I&rsquo;ve never used <strong>Let&rsquo;s Encrypt</strong> previously, I went through their process to request new certificates and am currently using them for the site. However, I&rsquo;ll likely switch to <strong>CloudFlare</strong> in future. This will avoid the complexity of automating regular certificate updates when using <strong>Let&rsquo;s Encrypt</strong> (90 day expiry versus 15 years with <strong>CloudFlare</strong>, not a difficult choice).</p>
<p>All in all, I&rsquo;m happy with my new setup and have learned a lot moving to GitLab Pages. As always, I hope you found this post useful, or at least interesting. To contact me or provide feedback, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<p>Thanks!</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>One final note, updating these dependencies in my site <code>Gemfile</code> has reduced the site build-time in my local test environment by 50%. I&rsquo;m not sure why this has occurred, but a happy side effect nonetheless.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item><item><title>Script for Exporting Micro.blog posts to Jekyll</title><link>/posts/2018/05/script-for-exporting-micro.blog-posts-to-jekyll/</link><pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate><guid>/posts/2018/05/script-for-exporting-micro.blog-posts-to-jekyll/</guid><description>As I&amp;rsquo;ve mentioned in previous posts, I&amp;rsquo;m a fan of Jekyll for building static websites, but I have also been experimenting with Micro.blog for my personal site. There was a recent discussion on Micro.blog looking for options to mirror posts from Micro.blog to Jekyll. This question stuck with me, and as I already have a Raspberry Pi server in production for handling other daily maintenance tasks, I decided to investigate if this could be done in Python, building on my previous experience.</description><content type="html"><![CDATA[<p>As I&rsquo;ve mentioned in previous posts, I&rsquo;m a fan of <a href="https://jekyllrb.com/" target="_blank">Jekyll</a>
 for building static websites, but I have also been experimenting with <a href="http://Micro.blog" target="_blank">Micro.blog</a>
 for my personal site. There was a recent discussion on <strong>Micro.blog</strong> looking for options to mirror posts from <strong>Micro.blog</strong> to Jekyll. This question stuck with me, and as I already have a Raspberry Pi server in production for handling other daily maintenance tasks, I decided to investigate if this could be done in Python, building on my previous experience.</p>
<p>There are several options for exporting posts from <strong>Micro.blog</strong>, such as <em>RSS</em>, <em>JSON</em> or the <em>MetaWeblog</em> API. From my research, I discovered the <em>MetaWeblog</em> API had one significant advantage over the other options. This API presents posts in their original <em>Markdown</em> source format, allowing for easy export to Jekyll without translation.</p>
<p>Also, for the purposes of simplicity, I built the script to export all posts from the past &lsquo;x&rsquo; hours, rather than tracking which posts have already been exported. This avoided the need for a backend database or log file to track post IDs. In the worst case, if a post was downloaded twice, it would simply over-write the matching post in Jekyll. Perhaps in future I will add this functionality, but I&rsquo;ll wait and see if it becomes an issue after long-term use.</p>
<p>Here is a breakdown of the current version of the script, highlighting the most important sections. I also posted the script in full <a href="https://github.com/bftsystems/micro-blog-to-jekyll" target="_blank">here</a>
 on Github, so you can download or submit code improvements. As always, the first section declares any external modules and functions required by the script.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#!/usr/bin/env python</span>

<span style="color:#f92672">import</span> xmlrpclib
<span style="color:#f92672">import</span> re<span style="color:#f92672">,</span> string<span style="color:#f92672">,</span> sys<span style="color:#f92672">,</span> time<span style="color:#f92672">,</span> urllib<span style="color:#f92672">,</span> urlparse
<span style="color:#f92672">from</span> datetime <span style="color:#f92672">import</span> datetime, timedelta

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">datetime_from_utc_to_local</span>(utc_datetime):
    now_timestamp <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
    offset <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>fromtimestamp(now_timestamp) <span style="color:#f92672">-</span> datetime<span style="color:#f92672">.</span>utcfromtimestamp(now_timestamp)
    <span style="color:#66d9ef">return</span> utc_datetime <span style="color:#f92672">+</span> offset
</code></pre></div><p>In the next section, all the <strong>Micro.blog</strong> specific settings are declared. Primarily, this includes the app token, full domain name, and the maximum number of posts to read. This variable should be set to the maximum number of posts you expect between scheduled runs of the script.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Micro.blog domain details</span>
app_token <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;xxxxxxxxxxxxxxxxxxxx&#39;</span>
<span style="color:#75715e"># enter Micro.blog or custom domain name (to properly handle Micro.blog img links)</span>
domain <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;www.thedeskofbrad.ca&#39;</span>
user_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;thedeskofbrad&#39;</span>
<span style="color:#75715e"># maximum number of posts expected in time range </span>
max_posts <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>

<span style="color:#75715e"># Micro.blog xml-rpc server endpoint</span>
server <span style="color:#f92672">=</span> xmlrpclib<span style="color:#f92672">.</span>ServerProxy(<span style="color:#e6db74">&#39;https://Micro.blog/xmlrpc&#39;</span>)
posts <span style="color:#f92672">=</span> server<span style="color:#f92672">.</span>metaWeblog<span style="color:#f92672">.</span>getRecentPosts(domain, user_id, app_token, max_posts)
</code></pre></div><p>As noted in the documentation, <strong>Micro.blog</strong> does not use passwords, the script uses an app token to authenticate connections. To generate an app token, open <strong>Settings</strong> on the <strong>Micro.blog</strong> site, and click the <strong>Edit Apps</strong> link at the bottom of the page. Enter a suitable name for the script, and click the <strong>Generate Token</strong> link. Once created, update the <em>app_token</em> variable in the script with this code.</p>

    <img src="Create-Micro-Blog-Token.jpg"  alt="Create Micro.blog Token"  class="left"  style="border-radius: 4px; margin-left: 2em;"  />


<p>The next section covers any Jekyll specific settings, such as the path to your local Jekyll repository, and the page style to be used for posts. Also included is the time range of posts to be exported by the script. On my system, the script is scheduled to run once daily, so the time range is set for 24 hours.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># jekyll blog path</span>
jekyll_root <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;/home/pi/git/thedeskofbrad.github.io&#39;</span>
jekyll_post_layout <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;single&#34;</span>

<span style="color:#75715e"># time range to scan posts</span>
time_range <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now() <span style="color:#f92672">-</span> timedelta(hours <span style="color:#f92672">=</span> <span style="color:#ae81ff">24</span>)
</code></pre></div><p>This last section does all the work of pulling each post from <strong>Micro.blog</strong>, checking for a title, and confirming if the post falls within the specified time range (correcting for UTC to local time). The script also downloads any image attached to the post and updates the <code>img</code> tag to use the Jekyll <code>site.url</code> variable. Currently, <strong>Micro.blog</strong> only allows for one image per post, which greatly simplifies the check to a single specifically formatted <code>img</code> tag (this is required, as the <strong>Micro.blog</strong> API does not use the <em>enclosure</em> field for attachments). Once all this is done, each post is exported to Jekyll.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> post <span style="color:#f92672">in</span> posts:
	<span style="color:#75715e"># Correct for UTC date returned from Micro.blog API</span>
	dateCreated <span style="color:#f92672">=</span> datetime_from_utc_to_local(datetime<span style="color:#f92672">.</span>strptime(str(post[<span style="color:#e6db74">&#39;dateCreated&#39;</span>])[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#39;%Y%m</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">T%H:%M:%S&#39;</span>))
	<span style="color:#75715e"># Download posts with titles in selected time range</span>
	<span style="color:#66d9ef">if</span> dateCreated <span style="color:#f92672">&gt;</span> time_range <span style="color:#f92672">and</span> post[<span style="color:#e6db74">&#39;title&#39;</span>] <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;&#34;</span>:
		title <span style="color:#f92672">=</span> post[<span style="color:#e6db74">&#39;title&#39;</span>]
		content <span style="color:#f92672">=</span> post[<span style="color:#e6db74">&#39;description&#39;</span>]
		
		<span style="color:#75715e"># If there is a Micro.blog img attribute, download the image to Jekyll blog assets folder</span>
		img <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>search(<span style="color:#e6db74">&#34;(?P&lt;url&gt;img src=</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">https?://&#34;</span><span style="color:#f92672">+</span>domain<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;[^</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">]+)&#34;</span>, content)
		<span style="color:#66d9ef">if</span> img <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
			img_url <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>group(<span style="color:#e6db74">&#34;url&#34;</span>)[<span style="color:#ae81ff">9</span>:]
			img_filename <span style="color:#f92672">=</span> img_url<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;/&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] 
			download_path <span style="color:#f92672">=</span> jekyll_root <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/assets/images/&#34;</span> <span style="color:#f92672">+</span> img_filename
			download <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>URLopener()
			download<span style="color:#f92672">.</span>retrieve(img_url, download_path)
			content <span style="color:#f92672">=</span> string<span style="color:#f92672">.</span>replace(content, img_url, <span style="color:#e6db74">&#39;{</span><span style="color:#e6db74">% r</span><span style="color:#e6db74">aw %}{{ site.url }}{{ site.baseurl }}{</span><span style="color:#e6db74">% e</span><span style="color:#e6db74">ndraw %}/assets/images/&#39;</span> <span style="color:#f92672">+</span> img_filename)

		jekyll_post_filename <span style="color:#f92672">=</span> dateCreated<span style="color:#f92672">.</span>strftime(<span style="color:#e6db74">&#34;%Y-%m-</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;-&#39;</span> <span style="color:#f92672">+</span> title<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34; &#34;</span>,<span style="color:#e6db74">&#34;-&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.md&#34;</span>		
		<span style="color:#66d9ef">with</span> open(jekyll_root <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/_posts/&#34;</span> <span style="color:#f92672">+</span> jekyll_post_filename, <span style="color:#e6db74">&#34;w&#34;</span>) <span style="color:#66d9ef">as</span> text_file:
			text_file<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;---</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
			text_file<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;layout: &#34;</span> <span style="color:#f92672">+</span> jekyll_post_layout <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
			text_file<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;title: &#34;</span> <span style="color:#f92672">+</span> title <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
			text_file<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;date: &#34;</span> <span style="color:#f92672">+</span> dateCreated<span style="color:#f92672">.</span>strftime(<span style="color:#e6db74">&#34;%Y-%m-</span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> %H:%M:%S&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
			text_file<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;---</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
			text_file<span style="color:#f92672">.</span>write(content<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;utf-8&#34;</span>))
</code></pre></div><p>As you can see, there isn&rsquo;t a lot of error checking in the code. I decided this was unnecessary as I assume the <strong>Micro.blog</strong> API presents &ldquo;clean&rdquo; data which has been validated as part of the posting process. Of course, someone could infiltrate their servers and corrupt the database, but I considered this a low risk for my purposes.</p>
<p>Lastly, to ensure the script runs once every 24 hours (matching the <em>time_range</em> specified in the script), I added it to the <strong>crontab</strong> file (shown below). This configuration runs the script daily at one minute after midnight. Using <strong>cron</strong> also has an added bonus, if issues are encountered by the script, the error output is included in the report emailed from <strong>cron</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># m h  dom mon dow   command</span>
<span style="color:#ae81ff">1</span> <span style="color:#ae81ff">0</span> * * * /home/pi/bin/micro-blog-to-jekyll.sh
</code></pre></div><p>One final point, this script simply downloads new posts from <strong>Micro.blog</strong> and saves them to the local Jekyll repository. Additional steps will be needed to force a rebuild of your site to actually publish any new posts. For my site using <a href="https://pages.github.com" target="_blank">Github Pages</a>
, I have a script which runs daily to perform a <code>git commit [...]</code>, and <code>git push origin master</code> to make any new posts live on my site.</p>
<p>I hope you found this post useful, or at least interesting. If you would like to contact me or provide feedback, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
]]></content></item><item><title>Hosting Media or Files on Amazon S3</title><link>/posts/2018/03/hosting-media-or-files-on-amazon-s3/</link><pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate><guid>/posts/2018/03/hosting-media-or-files-on-amazon-s3/</guid><description>As I&amp;rsquo;ve mentioned in previous posts, I&amp;rsquo;m a big fan of Jekyll for creating static websites. The one obvious shortcoming of Jekyll is the entire site must be regenerated when adding even a single post. Lately I&amp;rsquo;ve been experimenting with Micro.blog for personal posts not appropriate here. It has an iOS app for making quick updates from your phone, and hooks into Twitter and Facebook for automatic cross-posting to your social feeds.</description><content type="html"><![CDATA[<p>As I&rsquo;ve mentioned in previous posts, I&rsquo;m a big fan of <a href="https://jekyllrb.com/" target="_blank">Jekyll</a>
 for creating static websites. The one obvious shortcoming of Jekyll is the entire site must be regenerated when adding even a single post. Lately I&rsquo;ve been experimenting with <a href="http://micro.blog" target="_blank">Micro.blog</a>
 for personal posts not appropriate here. It has an iOS app for making quick updates from your phone, and hooks into Twitter and Facebook for automatic cross-posting to your social feeds. Yes, I could post directly on Twitter or Facebook, but I like hosting my content independently. That said, <a href="http://micro.blog" target="_blank">Micro.blog</a>
 does have one shortcoming in my use. To post anything more than just a simple photo, it must be hosted elsewhere and linked. Of course, I could post my media on any number of free services available, but I want to ensure the links don&rsquo;t break if their URL scheme changes, or they happen to go under. For this reason, I decided to host my files on Amazon&rsquo;s S3 service.</p>
<p>During my research, I found many posts out there on hosting content using Amazon S3, but none had a process that worked specifically for me. I&rsquo;ve documented here the steps I followed, mainly for my benefit, but I&rsquo;m hoping you will also find it useful.</p>
<p>First off, to use Amazon Web Services, you need an Amazon account. Assuming most everyone has an account, I&rsquo;m not covering those steps here. Nonetheless, if you don&rsquo;t have an account, you need to set one up before going further.</p>
<ol>
<li>
<p>Go to the <a href="https://aws.amazon.com" target="_blank">Amazon Web Services</a>
 page and sign into the AWS console using your account<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
</li>
<li>
<p>Once logged in, under <strong>Services</strong>, select <strong>S3</strong>.</p>
</li>
<li>
<p>On the <strong>S3</strong> page, click on the button labelled <strong>Create Bucket</strong>.</p>

    <img src="AWS_S3_Create_Bucket.png"  alt="AWS S3 Create Bucket"  class="left"  style="border-radius: 4px;"  />


</li>
<li>
<p>Enter a bucket name and select an appropriate region for hosting, then click <strong>Next</strong>. In this example, I have used <code>files.bftsystems.ca.</code>  Any name could do, but this makes it easy for me to remember what URL links to this bucket.</p>

    <img src="AWS_S3_Bucket_Name.png"  alt="AWS S3 Bucket Name"  class="left"  style="border-radius: 4px; width: 75%;"  />


</li>
<li>
<p>You will be prompted for properties to associate with this bucket. As this will be used for simple hosting of public files, no special properties are needed. Just click <strong>Next</strong> to accept the defaults.</p>
</li>
<li>
<p>You will now be prompted to select permissions for this bucket. The only setting to be changed are the public permissions. Change this to <strong>Grant public read access to this bucket</strong> and click <strong>Next</strong>.</p>

    <img src="AWS_S3_Public_Permissions.png"  alt="AWS S3 Public Permissions"  class="left"  style="border-radius: 4px; width: 75%;"  />


</li>
<li>
<p>Lastly, a summary of the bucket settings will be shown. Confirm everything, then click <strong>Create bucket</strong>.</p>
</li>
<li>
<p>Once the bucket is created, it will appear on the S3 page. Click the bucket name in the list to open it.</p>

    <img src="AWS_S3_Bucket_List.png"  alt="AWS S3 Bucket List"  class="left"  style="border-radius: 4px; width: 75%;"  />


</li>
<li>
<p>When the bucket opens, click the <strong>Upload</strong> button to begin adding files. There are many tools available to simplify uploading of files to S3<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, but for the purposes of this tutorial, I&rsquo;ll cover the steps to upload using the website.</p>

    <img src="AWS_S3_Upload.png"  alt="AWS S3 Upload"  class="left"  style="border-radius: 4px;"  />


</li>
<li>
<p>When prompted, click the <strong>Add files</strong> link to open a file browser and select the files to upload, or drag and drop the files to the browser window. Once all the files to upload are in the list, click <strong>Next</strong> to begin the upload process.</p>
</li>
<li>
<p>Once again, when prompted for permissions, select <strong>Grant public read access to these object(s)</strong> to allow access from the web.</p>
</li>
<li>
<p>As no special properties are required, you can safely click the <strong>Upload</strong> button at this point to skip the remaining steps.</p>
</li>
<li>
<p>When the upload completes, the files will appear in the bucket list (pun intended), but they are not yet publicly accessible. To enable this, select the files, then click the <strong>Make public</strong> option under the <strong>More</strong> button. You will see a warning the objects will now be made public. Click the <strong>Make public</strong> button to proceed.</p>

    <img src="AWS_S3_Make_Public.png"  alt="AWS S3 Make Public"  class="left"  style="border-radius: 4px;"  />


</li>
</ol>
<p>Once the steps above are completed, all the files uploaded will be accessible using the Amazon S3 URL at <strong>files.bftsystems.ca.s3.amazonaws.com</strong>. That works, but I would prefer the URL be a sub-domain of my site. This can be done by adding a <strong>CNAME</strong> record for my domain pointing to the URL of the S3 bucket. How this is done will vary depending on your web host, but here is the screenshot from my web host console.</p>

    <img src="AWS_Create_CNAME_Record.png"  alt="AWS Create CNAME Record"  class="left"  style="border-radius: 4px; width: 75%; margin-left: 2em;"  />


<p>Once this DNS record has been added, all the files I uploaded can now be linked at the URL <strong>files.bftsystems.ca</strong>, totally transparent to my readers.</p>
<p>I have one last recommendation. Although Amazon S3 rates are exceedingly low, they do charge by usage<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. To avoid web crawlers indexing my S3 buckets and racking up charges, I always add a <code>robots.txt</code> file with the following text to ensure the bucket is not indexed (at least by web spiders that follow the rules).</p>
<pre><code>User-agent: *
Disallow: /
</code></pre>
<p>I hope you found this post useful, or at least interesting. If you would like to contact me or provide feedback, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>As a reminder, your Amazon credentials will have full root access to all your AWS resources, so use a strong password. I also recommend using two-factor authentication for added security. See the steps <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_virtual.html" target="_blank">here</a>
 to enable Multi-factor authentication in AWS.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>I personally recommend <a href="https://www.panic.com/transmit" target="_blank">Transmit</a>
 by Panic. It is available for both macOS and iOS.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>As Amazon S3 is a usage based service, I also enable Billing Alerts and PDF invoices by email to keep a close eye on usage. See the steps <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html" target="_blank">here</a>
 to enable these features.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item><item><title>Open New Terminal at Folder in macOS</title><link>/posts/2017/10/open-new-terminal-at-folder-in-macos/</link><pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate><guid>/posts/2017/10/open-new-terminal-at-folder-in-macos/</guid><description>Previously when I was a Windows user (back in the dark times), I used the Command Prompt Here tool from the Windows 95 PowerToys collection offered by Microsoft. When I transitioned to OS X, I was disappointed there was not a similar option or tool available. Of course, after some research I discovered this was built into the system. I just needed to look in the right place.
To add this function in Finder, open System Preferences, Keyboard, then click on the Shortcuts tab.</description><content type="html"><![CDATA[<p>Previously when I was a Windows user (back in the dark times), I used the <strong>Command Prompt Here</strong> tool from the Windows 95 PowerToys collection offered by Microsoft. When I transitioned to OS X, I was disappointed there was not a similar option or tool available. Of course, after some research I discovered this was built into the system. I just needed to look in the right place.</p>
<p>To add this function in Finder, open System Preferences, Keyboard, then click on the Shortcuts tab. Scroll through the list until you find the <em>New Terminal at Folder</em> option. Once this is checked, it will now be available under <em>Services</em> in Finder when you right-click on a folder. Also, while you are in this tab, I recommend you review all the options in the list, as there are many interesting tools to be found here.</p>

    <img src="new_terminal_tab.jpg"  alt="New Terminal Tab at Folder"  class="left"  style="border-radius: 4px; margin-left: 2em;"  />


<p>As you can see in the screenshot, I have checked the <em>New Terminal Tab at Folder</em> option (my preferred setting). With this option, if there is no Terminal window open, it opens a new shell at the folder. If Terminal is already open, the shell is opened in a new tab, rather than a separate window. Here is how the option appears in Finder when you right-click on a folder.</p>

    <img src="finder_services.jpg"  alt="Finder Services"  class="center"  style="border-radius: 4px; margin-left: 2em;"  />


<p>Now that you&rsquo;ve know how to open a folder directly in Terminal, you may have noticed it opens using the default macOS shell (black text on white background). If you are like me and prefer a different shell, you must change several settings to ensure Terminal always uses your preferred options. My personal preference is the <em>Homebrew</em> shell, as I grew up using green screen terminals, and I find the green text on black background easier on my eyes. To ensure Terminal opens using your preferred shell, look under Terminal Preferences, General, select <em>On startup, open: New window with profile</em> and select the profile you wish to use.</p>

    <img src="terminal_startup_profile.jpg"  alt="Terminal Startup Profile"  class="center"  style="border-radius: 5px; margin-left: 2em;"  />


<p>Interestingly, this does not change the <em>default</em> options. If you open a separate Terminal window or tab, it will still use the macOS default shell. To change the default, look under Terminal Preferences, Profiles, select your preferred shell, then click the <em>Default</em> button at the bottom of the list. This will ensure all new shells open using your default options.</p>

    <img src="change_default_shell.jpg"  alt="Change Default Shell"  class="left"  style="border-radius: 4px; margin-left: 2em;"  />


<p>There are many other Terminal options available, I suggest you take the time to play around and see what works best for you. I plan to cover more macOS Terminal tricks in future posts.</p>
<p>If you would like to contact me or provide feedback on this post, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
. Thanks!</p>
]]></content></item><item><title>Hiding Applications from Spotlight in macOS</title><link>/posts/2017/09/hiding-applications-from-spotlight-in-macos/</link><pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate><guid>/posts/2017/09/hiding-applications-from-spotlight-in-macos/</guid><description>I have been using OS X (now macOS) for years, yet this simple trick somehow alluded me. Regularly, I would use the standard command + space combination to open Spotlight search, then type photo and press enter, and for whatever reason Photo Booth would open. Similarly, typing type text in Spotlight would open TextEdit instead of Textastic (my current text editor of choice). This was both annoying and perplexing, as I rarely used either application.</description><content type="html"><![CDATA[<p>I have been using OS X (now macOS) for years, yet this simple trick somehow alluded me. Regularly, I would use the standard <code>command</code> + <code>space</code> combination to open Spotlight search, then type <code>photo</code> and press <code>enter</code>, and for whatever reason Photo Booth would open. Similarly, typing type <code>text</code> in Spotlight would open TextEdit instead of Textastic (my current text editor of choice). This was both annoying and perplexing, as I rarely used either application. Why would Spotlight suddenly choose these applications instead? I tried the obvious solution of trashing the apps, but would see the standard macOS warning about deleting an application which is required by macOS.</p>

    <img src="application_required.png"  alt="Application required by macOS"  class="left"  style="border-radius: 4px; margin-left: 2em;"  />


<p>Of course, I could go ahead and use <code>rm -r</code> to delete the application from terminal, but one never knows what hooks are in the backend of macOS. Being a lazy <a href="http://www.thegeekstuff.com/2011/07/lazy-sysadmin/" target="_blank">System Administrator</a>
, it was never aggravating enough to the point of researching a solution, so I just lived with it&hellip;until now. In the most recent iOS release (iOS 11), Apple finally provided the ability to delete Photo Booth on the iPad (hallelujah!). This alone motivated me to search for a solution to the issue. Little did I know how simple and obvious it would be.</p>
<p>To permanently remove an app from Spotlight search, open System Preferences, Spotlight, then click the Privacy tab. In this window, click the <code>+</code> button and browse to the Applications folder and select the application to hide. Alternatively, open the Applications folder in Finder and drag the application to the window. Once an application has been added to the list, it may take a moment for Spotlight to update the index, but shortly you will notice the application no longer appears in searches.</p>

    <img src="spotlight_privacy_pane.png"  alt="Spotlight Privacy pane"  class="left"  style="border-radius: 4px; margin-left: 2em;"  />


<p>Seriously, how did I miss this for all these years? Either way, it&rsquo;s fixed and I&rsquo;m happy, that&rsquo;s what matters, no? If you would like to contact me or provide feedback on this post, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
. Thanks!</p>
]]></content></item><item><title>Binding Automator Scripts to Shortcut keys in macOS</title><link>/posts/2017/08/binding-automator-scripts-to-shortcut-keys-in-macos/</link><pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate><guid>/posts/2017/08/binding-automator-scripts-to-shortcut-keys-in-macos/</guid><description>For years after moving from Windows to Mac, I used Karabiner to make my trusty Logitech S510 keyboard act normal in OS X, at least normal from an ex-Windows users perspective. Karabiner also had the added benefit of allowing a standard PC keyboard to mimic other keys, such as the MacBook Eject key. For example, I could bind the eject function to Print Screen, allowing Ctrl + Shift + Print Screen to lock my MacBook quickly from the keyboard.</description><content type="html"><![CDATA[<p>For years after moving from Windows to Mac, I used <a href="https://karabiner-elements.pqrs.org" target="_blank">Karabiner</a>
 to make my trusty Logitech S510 keyboard act normal in OS X, at least normal from an ex-Windows users perspective.  Karabiner also had the added benefit of allowing a standard PC keyboard to mimic other keys, such as the MacBook <code>Eject</code> key.  For example, I could bind the eject function to <code>Print Screen</code>, allowing <code>Ctrl</code> + <code>Shift</code> + <code>Print Screen</code> to lock my MacBook quickly from the keyboard.</p>
<p>After a time, my Windows habits slowly faded and OS X became first nature to me.  I found myself rarely using any of the PC key-bindings, preferring native Mac functionality.  The final straw came with macOS Sierra, which broke Karabiner once and for all.  The only feature I missed was the ability to bind PC keys to functions such as <code>Eject</code>.  Yes, there are other utilities available, such as <a href="https://www.keyboardmaestro.com/main/" target="_blank">Keyboard Maestro</a>
, but they seemed like overkill, and I didn&rsquo;t want any external dependencies.</p>
<p>After some research, I discovered the ability in macOS to bind Automator scripts to a shortcut in the Keyboard System Preferences panel.  For example, I could create a simple Automator script to run the following AppleScript block.  This script will open the standard macOS shutdown prompt as if I had pressed <code>Ctrl</code> + <code>Eject</code> on my MacBook keyboard.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-applescript" data-lang="applescript"><span style="color:#66d9ef">on</span> run {input, parameters}
	<span style="color:#66d9ef">tell</span> application <span style="color:#e6db74">&#34;loginwindow&#34;</span> <span style="color:#66d9ef">to</span> «event aevtrsdn»
<span style="color:#66d9ef">end</span> run
</code></pre></div><p>Once I saved this Automator script in the Services folder (<code>~/Library/Services/</code>), it could be bound to a shortcut key combination in Keyboard Preferences under Shortcuts, Services.  See the screenshot below for an example of several I have setup on my own system.</p>

    <img src="automator_services.jpg"  alt="Keyboard Services System Preferences"  class="center"  style="border-radius: 4px; width: 95%;"  />


<p>Another advantage to this solution is that it works everywhere, whether I&rsquo;m working on my MacBook keyboard, or any old PC keyboard I happen to have plugged in.</p>
<p>If you would like to contact me or provide feedback on this post, use the <a href="/contact">Contact</a>
 page, or message me on <a href="http://twitter.com/bftsystems" target="_blank">Twitter</a>
. Thanks!</p>
]]></content></item></channel></rss>